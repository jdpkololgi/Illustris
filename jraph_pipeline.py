import os
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"
import time
import pickle
from datetime import datetime
import numpy as np
import pandas as pd
import networkx as nx
import torch # Used for compatibility with existing data structures if needed
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

import jax
import jax.numpy as jnp
import jraph
import haiku as hk
import optax

from Network_stats import network
from graph_net_models import make_graph_network
from utils import preprocess_features

# Set up JAX to use 64-bit precision if needed, though 32 is usually fine for ML
# jax.config.update("jax_enable_x64", True)


def convert_pyg_to_jraph(pyg_data):
    """
    Convert PyTorch Geometric Data object to Jraph GraphsTuple.
    """
    print("Converting PyG Data to Jraph GraphsTuple...")
    
    # Nodes
    # pym_data.x is [N, F]
    node_features = jnp.array(pyg_data.x.numpy(), dtype=jnp.float32)
    
    # Edges
    # pyg_data.edge_index is [2, E]
    # jraph expects senders, receivers
    senders = jnp.array(pyg_data.edge_index[0].numpy(), dtype=jnp.int32)
    receivers = jnp.array(pyg_data.edge_index[1].numpy(), dtype=jnp.int32)
    
    # Edge Features
    # pyg_data.edge_attr is [E, F_edge] or [E]
    if pyg_data.edge_attr is not None:
        edge_attr = pyg_data.edge_attr.numpy()
        # If it's 1D, reshape to [E, 1]
        if len(edge_attr.shape) == 1:
            edge_attr = edge_attr.reshape(-1, 1)
        edge_features = jnp.array(edge_attr, dtype=jnp.float32)
    else:
        # Default edge features if missing
        edge_features = jnp.ones((len(senders), 1), dtype=jnp.float32)
        
    # Labels
    # pyg_data.y
    labels = jnp.array(pyg_data.y.numpy(), dtype=jnp.int32)
    
    # Masks (train_mask, val_mask, test_mask are bool tensors in pyg_data)
    train_mask = jnp.array(pyg_data.train_mask.numpy(), dtype=bool)
    val_mask = jnp.array(pyg_data.val_mask.numpy(), dtype=bool)
    test_mask = jnp.array(pyg_data.test_mask.numpy(), dtype=bool)
    
    # Graph Tuple
    n_node = jnp.array([node_features.shape[0]])
    n_edge = jnp.array([senders.shape[0]])
    
    graph = jraph.GraphsTuple(
        nodes=node_features,
        edges=edge_features,
        senders=senders,
        receivers=receivers,
        n_node=n_node,
        n_edge=n_edge,
        globals=None
    )
    
    return graph, labels, (train_mask, val_mask, test_mask)

def load_data(masscut=1e9, cache_path=None):
    """
    Load data from cache if available, otherwise generate it.
    """
    if cache_path is None:
        cache_path = f"processed_gcn_data_mc{masscut:.0e}.pt"
        
    if os.path.exists(cache_path):
        print(f"Loading cached data from {cache_path}...")
        try:
            # torch.load requires the classes to be available. 
            # We assume torch_geometric is installed as gcn_pipeline uses it.
            data_tuple = torch.load(cache_path, weights_only=False) # (netx_geom, features, targets)
            netx_geom = data_tuple[0]
            # features/targets are also in tuple but netx_geom has what we need
            
            # The cached netx_geom might already have Scaled features if generated by gcn_pipeline?
            # gcn_pipeline line 96: features = preprocess_features(features)
            # line 100: netx_geom.x = torch.tensor(features.values...)
            # So yes, x is already scaled.
            
            return convert_pyg_to_jraph(netx_geom)
        except Exception as e:
            print(f"Failed to load cache: {e}. Falling back to generation.")
    
    # Fallback to generation (Logic from previous get_node_features_and_targets)
    print("Generating data from scratch...")
    testcat = network(masscut=masscut, from_DESI=False)
    testcat.cweb_classify(xyzplot=False)
    testcat.network_stats_delaunay(buffer=False)
    G = testcat.subhalo_delauany_network(xyzplot=False)
    
    features = testcat.data.iloc[:, :-1]
    targets = testcat.data.iloc[:, -1]
    
    # Scale
    features_df = preprocess_features(features)
    
    # Create manual PyG-like object or just direct conversion to Jraph
    # We'll just build the graph manually here to match the return signature
    # Since we need masks, we recreate mask logic
    
    # ... (Reusing logic for masks from creating_masks)
    # To save space in this function, we'll verify if we can skip full generation if cache exists usually.
    # But for robustness, let's keep it simple.
    
    # We will raise error or simple implementation for now to keep this block concise, 
    # as the user strongly implied cache should exist. 
    # Re-implementing the full mask logic here inside the fallback is repetitive.
    # Let's assume cache exists or fail with instruction to run gcn_pipeline to generate cache?
    # No, better to be self contained.
    
    # For now, I'll put the "Manual" generation logic in a simplified way or rely on cache.
    # User said "have a look at cached files".
    
    raise FileNotFoundError(f"Cache file {cache_path} not found. Please run gcn_pipeline.py to generate data first or ensure cache exists.")


def calculate_class_weights(targets):
    """Calculate class weights using sklearn."""
    try:
        classes = np.unique(targets)
        weights = compute_class_weight(class_weight='balanced', classes=classes, y=targets)
        return jnp.array(weights, dtype=jnp.float32)
    except Exception as e:
        print(f"Warning: Could not calculate class weights: {e}")
        return jnp.ones(4, dtype=jnp.float32)

# --- Modeling ---

def loss_fn(params, graph, labels, mask, net_apply, rng, class_weights):
    """
    Cross Entropy Loss masked by 'mask'.
    """
    # Run model
    # net.apply takes (params, rng, graph)
    # output graph.nodes is [num_nodes, num_classes] (logits)
    output_graph = net_apply(params, rng, graph)
    logits = output_graph.nodes
    
    # Logits [N, 4], Labels [N]
    # One-hot labels
    one_hot_labels = jax.nn.one_hot(labels, num_classes=4)
    
    # Calculate weighted cross entropy
    # -sum(weight * label * log(softmax(logit)))
    # optax.softmax_cross_entropy takes logits and labels (one-hot or probs)
    loss_per_node = optax.softmax_cross_entropy(logits, one_hot_labels)
    
    # Weight by class imbalance
    # class_weights[labels] gives weight for each node's true class
    # Convert scalar class weights to shape [N]
    node_weights = class_weights[labels]
    
    weighted_loss = loss_per_node * node_weights
    
    # Apply mask (only train on train nodes)
    masked_loss = weighted_loss * mask
    
    # Average over number of masked nodes (avoiding divide by zero)
    num_masked = jnp.sum(mask)
    final_loss = jnp.sum(masked_loss) / jnp.maximum(num_masked, 1.0)
    
    return final_loss, (logits, num_masked)

def compute_accuracy(logits, labels, mask):
    preds = jnp.argmax(logits, axis=-1)
    correct = (preds == labels) & mask
    accuracy = jnp.sum(correct) / jnp.maximum(jnp.sum(mask), 1.0)
    return accuracy

def main():
    print(f"JAX Devices: {jax.devices()}")
    num_devices = jax.local_device_count()
    print(f"Running on {num_devices} devices.")
    
    # 1. Load Data
    masscut = 1e9
    graph, labels, (train_mask, val_mask, test_mask) = load_data(masscut=masscut)
    
    # Calculate class weights - labels is a jnp array now
    # We need to pass original targets array/series or just use the labels for weight calc
    # load_data returns jax array for labels.
    # We can convert back to numpy for sklearn weight calc
    class_weights = calculate_class_weights(np.array(labels))
    
    print(f"Graph stats: Nodes={graph.n_node[0]}, Edges={graph.n_edge[0]}")
    print(f"Train size: {jnp.sum(train_mask)}, Val size: {jnp.sum(val_mask)}, Test size: {jnp.sum(test_mask)}")
    
    # 2. Model Setup
    # Initialize network
    # make_graph_network returns a function that takes a graph
    # Reduced passes to 2 to prevent OOM on single GPU - RESTORED to 4 for Compute Node
    net_fn = make_graph_network(num_passes=4)
    net = hk.transform(net_fn) # hk.transform converts a function using Haiku modules into an (init, apply) pair.
    
    # Init params (single device for init)
    rng = jax.random.PRNGKey(42)
    # We need a dummy graph with correct shape for initialization
    # Jraph graphs can be batched/padded, but here we use the full graph logic.
    params = net.init(rng, graph)
    
    # 3. Optimizer
    # Similar schedule to gcn_pipeline if possible, or simple AdamW
    lr = 1e-3
    optimizer = optax.adamw(lr)
    opt_state = optimizer.init(params)
    
    # 4. Replicate Data to Devices for PMAP
    # We replicate the GRAPH and PARAMS across devices.
    # We SHARD the MASKS so each device computes loss on a subset of nodes.
    
    # Replicate params
    replicated_params = jax.device_put_replicated(params, jax.local_devices())
    
    # 3. Optimizer
    # GCN uses 3e-3. We'll use a schedule.
    num_epochs = 4000
    lr_schedule = optax.warmup_cosine_decay_schedule(
        init_value=1e-5,
        peak_value=3e-3, # Match GCN peak
        warmup_steps=500,
        decay_steps=num_epochs,
        end_value=1e-5
    )
    optimizer = optax.adamw(lr_schedule)
    opt_state = optimizer.init(params)
    
    # Replicate optimizer state
    replicated_opt_state = jax.device_put_replicated(opt_state, jax.local_devices())
    
    # Replicate graph (Data Parallelism usually shards data, but for Transductive on One Graph
    # where the graph fits in memory, replicating the graph structure is common 
    # so every node can see its neighbors. We only split the *supervision* signal).
    # stack along first dimension (device dim)
    # jraph definitions are not easily stackable with jnp.stack directly if contents are variable length,
    # we replicate the graph and shard the MASK).
    # Since we have only 1 graph, we just expand it to [num_devices, ...]
    replicated_graph = jax.device_put_replicated(graph, jax.local_devices())
    
    # Replicate labels
    replicated_labels = jax.device_put_replicated(labels, jax.local_devices())
    replicated_class_weights = jax.device_put_replicated(class_weights, jax.local_devices())

    # Shard the Train Mask
    # We want to split train_mask indices among devices
    # Create N unique masks.
    num_devices = len(jax.local_devices())
    train_indices = jnp.where(train_mask)[0]
    # split indices
    train_indices_sharded = jnp.array_split(train_indices, num_devices)
    
    # Create masks for each shard
    # This must be a static array per device, so we stack them.
    sharded_train_masks_list = []
    sharded_val_masks_list = [] # For parallel validation
    
    # Full masks
    full_val_mask = val_mask
    val_indices = jnp.where(full_val_mask)[0]
    val_indices_sharded = jnp.array_split(val_indices, num_devices)

    for i in range(num_devices):
        # Train mask shard
        m_train = jnp.zeros_like(train_mask)
        if len(train_indices_sharded[i]) > 0:
            m_train = m_train.at[train_indices_sharded[i]].set(True)
        sharded_train_masks_list.append(m_train)

        # Val mask shard
        m_val = jnp.zeros_like(full_val_mask)
        if len(val_indices_sharded[i]) > 0:
            m_val = m_val.at[val_indices_sharded[i]].set(True)
        sharded_val_masks_list.append(m_val)
        
    sharded_train_masks = jnp.stack(sharded_train_masks_list) # [Devices, Nodes]
    sharded_train_masks = jax.device_put_sharded(list(sharded_train_masks), jax.local_devices())

    sharded_val_masks = jnp.stack(sharded_val_masks_list)
    sharded_val_masks = jax.device_put_sharded(list(sharded_val_masks), jax.local_devices())

    # 4. Training Functions
    def loss_fn(params, graph, labels, mask, net_apply, rng, class_weights):
        # Pass is_training=True for Dropout
        logits = net_apply(params, rng, graph, is_training=True).nodes
        
        # Cross Entropy Loss
        # logits: [N, C], labels: [N]
        # one_hot labels
        labels_one_hot = jax.nn.one_hot(labels, num_classes=4)
        
        # optax loss
        per_node_loss = optax.softmax_cross_entropy(logits, labels_one_hot)
        
        # Apply class weights
        weights = jnp.take(class_weights, labels)
        weighted_loss = per_node_loss * weights
        
        # Mask
        masked_loss = weighted_loss * mask
        
        # Mean over masked nodes
        # Avoid division by zero
        num_masked = jnp.sum(mask)
        loss = jnp.sum(masked_loss) / jnp.maximum(num_masked, 1.0)
        
        return loss, (logits, num_masked)

    # Update Function
    def update(params, opt_state, graph, labels, mask, rng, class_weights):
        # rng mix
        step_rng = jax.random.fold_in(rng, jax.lax.axis_index('i'))
        
        # Gradients
        (train_loss, (logits, num_masked)), grads = jax.value_and_grad(loss_fn, has_aux=True)(
            params, graph, labels, mask, net.apply, step_rng, class_weights
        )
        
        # Sync gradients across devices (average)
        grads = jax.lax.pmean(grads, axis_name='i')
        
        # Sync Loss metrics for reporting
        # We sum the loss*count from each device and divide by total count
        total_loss_part = train_loss * num_masked
        total_count = jax.lax.psum(num_masked, axis_name='i')
        global_loss = jax.lax.psum(total_loss_part, axis_name='i') / jnp.maximum(total_count, 1.0)
        
        # Calculate Training Accuracy
        preds = jnp.argmax(logits, axis=-1)
        correct = (preds == labels) & mask
        
        total_correct = jax.lax.psum(jnp.sum(correct), axis_name='i')
        # total_count matches total_mask roughly, but num_masked is float from loss_fn?
        # loss_fn returns num_masked = jnp.sum(mask)
        # So total_count is correct denominator.
        global_acc = total_correct / jnp.maximum(total_count, 1.0)
        
        updates, new_opt_state = optimizer.update(grads, opt_state, params)
        new_params = optax.apply_updates(params, updates)
        
        return new_params, new_opt_state, global_loss, global_acc
        
    update = jax.pmap(update, axis_name='i')

    def evaluate(params, graph, labels, mask, rng, class_weights):
        # We can evaluate in parallel too
        step_rng = jax.random.fold_in(rng, jax.lax.axis_index('i'))
        
        # Pass is_training=False to disable Dropout
        # We need to manually invoke the model here because loss_fn assumes training?
        # Re-implementing evaluating part of loss_fn without gradients:
        
        logits = net.apply(params, step_rng, graph, is_training=False).nodes
        
        labels_one_hot = jax.nn.one_hot(labels, num_classes=4)
        per_node_loss = optax.softmax_cross_entropy(logits, labels_one_hot)
        weights = jnp.take(class_weights, labels)
        masked_loss = per_node_loss * weights * mask
        
        num_masked = jnp.sum(mask)
        loss = jnp.sum(masked_loss) / jnp.maximum(num_masked, 1.0)
        
        # Accuracy
        preds = jnp.argmax(logits, axis=-1)
        correct = (preds == labels) & mask
        
        total_correct = jax.lax.psum(jnp.sum(correct), axis_name='i')
        total_mask = jax.lax.psum(jnp.sum(mask), axis_name='i')
        
        accuracy = total_correct / jnp.maximum(total_mask, 1.0)
        
        # Global loss logic
        total_loss_part = loss * num_masked
        total_count = jax.lax.psum(num_masked, axis_name='i')
        global_loss = jax.lax.psum(total_loss_part, axis_name='i') / jnp.maximum(total_count, 1.0)
        
        return global_loss, accuracy
        
    evaluate = jax.pmap(evaluate, axis_name='i')

    # 5. Training Loop
    # num_epochs defined above
    report_every = 1
    
    # Random keys for each step (replicated)
    # We can just pass a single key and fold inside, 
    # but strictly we should shard RNGs if we want different randomness per device (dropout etc)
    # Here we fold inside 'update' with axis index, so single key is fine to start.
    
    current_rng = jax.random.PRNGKey(0)
    
    print("Starting training...")
    t0 = time.time()
    
    for epoch in range(num_epochs):
        current_rng, step_rng = jax.random.split(current_rng)
        # Replicate RNG
        step_rngs = jax.device_put_replicated(step_rng, jax.local_devices())
        
        replicated_params, replicated_opt_state, train_loss, train_acc = update(
            replicated_params, replicated_opt_state, 
            replicated_graph, replicated_labels, sharded_train_masks, 
            step_rngs, replicated_class_weights
        )
        
        # Validation
        if epoch % report_every == 0:
            val_loss, val_acc = evaluate(
                replicated_params, replicated_graph, replicated_labels, 
                sharded_val_masks, step_rngs, replicated_class_weights
            )
            # take first device result (they are identical due to pmean/psum)
            print(f"Epoch {epoch} | Train Loss: {train_loss[0]:.4f} | Train Acc: {train_acc[0]*100:.3f}% | Val Loss: {val_loss[0]:.4f} | Val Acc: {val_acc[0]*100:.3f}%")

    print(f"Training finished in {time.time() - t0:.2f}s")
    
    # Save Model
    # Take params from first device
    final_params = jax.device_get(jax.tree_map(lambda x: x[0], replicated_params))
    
    date_str = datetime.now().strftime("%Y-%m-%d")
    save_path = f'trained_jraph_model_{date_str}.pkl'
    with open(save_path, 'wb') as f:
        pickle.dump(final_params, f)
    print(f"Model saved to {save_path}")

if __name__ == '__main__':
    main()
